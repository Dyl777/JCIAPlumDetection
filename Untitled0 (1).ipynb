{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install ultralytics flask kagglehub torch torchvision imagehash albumentations scikit-learn pandas numpy seaborn matplotlib sympy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Olw2FH32mvBX",
        "outputId": "4d98e041-6f16-4954-9430-b7254c7d84c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ultralytics in /usr/local/lib/python3.11/dist-packages (8.3.109)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.11/dist-packages (3.1.0)\n",
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.11/dist-packages (0.3.11)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: imagehash in /usr/local/lib/python3.11/dist-packages (4.3.2)\n",
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.11/dist-packages (2.0.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (1.13.1)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.11.0.86)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.14.1)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.14)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from flask) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from flask) (3.1.6)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from flask) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from flask) (8.1.8)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from flask) (1.9.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from kagglehub) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy) (1.3.0)\n",
            "Requirement already satisfied: PyWavelets in /usr/local/lib/python3.11/dist-packages (from imagehash) (1.8.0)\n",
            "Requirement already satisfied: pydantic>=2.9.2 in /usr/local/lib/python3.11/dist-packages (from albumentations) (2.11.3)\n",
            "Requirement already satisfied: albucore==0.0.23 in /usr/local/lib/python3.11/dist-packages (from albumentations) (0.0.23)\n",
            "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.11/dist-packages (from albumentations) (4.11.0.86)\n",
            "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.23->albumentations) (3.12.3)\n",
            "Requirement already satisfied: simsimd>=5.9.2 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.23->albumentations) (6.2.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1.2->flask) (3.0.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (0.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade sympy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "id": "am9Wopb7DJa3",
        "outputId": "d12a1b2d-437c-4995-e50d-30ec26a45644"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (1.13.1)\n",
            "Collecting sympy\n",
            "  Downloading sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy) (1.3.0)\n",
            "Downloading sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m73.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sympy\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.13.1\n",
            "    Uninstalling sympy-1.13.1:\n",
            "      Successfully uninstalled sympy-1.13.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires sympy==1.13.1; python_version >= \"3.9\", but you have sympy 1.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed sympy-1.13.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "sympy"
                ]
              },
              "id": "76d33752271b4fe79b75d6bdddb546f5"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import hashlib\n",
        "from PIL import Image\n",
        "import shutil\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.swa_utils import AveragedModel, SWALR\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from collections import defaultdict\n",
        "import kagglehub\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Duplicate Removal\n",
        "def calculate_image_hash(image_path):\n",
        "    try:\n",
        "        with Image.open(image_path) as img:\n",
        "            img = img.convert('RGB')\n",
        "            img_bytes = img.tobytes()\n",
        "            return hashlib.md5(img_bytes).hexdigest()\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def remove_duplicate_images(image_dir):\n",
        "    hash_to_files = defaultdict(list)\n",
        "    class_counts_before = defaultdict(int)\n",
        "    class_counts_after = defaultdict(int)\n",
        "    class_names = ['bruised', 'cracked', 'rotten', 'spotted', 'unaffected', 'unripe']\n",
        "\n",
        "    for root, _, files in os.walk(image_dir):\n",
        "        class_name = os.path.basename(root)\n",
        "        if class_name in class_names:\n",
        "            for file in files:\n",
        "                if file.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\n",
        "                    file_path = os.path.join(root, file)\n",
        "                    image_hash = calculate_image_hash(file_path)\n",
        "                    if image_hash:\n",
        "                        hash_to_files[image_hash].append((file_path, class_name))\n",
        "                        class_counts_before[class_name] += 1\n",
        "\n",
        "    duplicates_found = 0\n",
        "    kept_files = []\n",
        "    for image_hash, file_info in hash_to_files.items():\n",
        "        if len(file_info) > 1:\n",
        "            duplicates_found += len(file_info) - 1\n",
        "            kept_files.append(file_info[0][0])\n",
        "            class_counts_after[file_info[0][1]] += 1\n",
        "            for file_path, _ in file_info[1:]:\n",
        "                try:\n",
        "                    os.remove(file_path)\n",
        "                except Exception:\n",
        "                    pass\n",
        "        else:\n",
        "            kept_files.append(file_info[0][0])\n",
        "            class_counts_after[file_info[0][1]] += 1\n",
        "\n",
        "    # Calculate duplicate proportions\n",
        "    duplicate_proportions = {}\n",
        "    for class_name in class_names:\n",
        "        before = class_counts_before.get(class_name, 0)\n",
        "        after = class_counts_after.get(class_name, 0)\n",
        "        if before > 0:\n",
        "            proportion = (before - after) / before * 100\n",
        "        else:\n",
        "            proportion = 0\n",
        "        duplicate_proportions[class_name] = proportion\n",
        "\n",
        "    print(f\"Total duplicates removed: {duplicates_found}\")\n",
        "    print(f\"Images retained: {len(kept_files)}\")\n",
        "    print(\"Sample Cleaned Data (images per class):\")\n",
        "    for class_name in class_names:\n",
        "        print(f\"{class_name}: {class_counts_after.get(class_name, 0)} images\")\n",
        "\n",
        "    # Plot duplicate proportions\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.bar(duplicate_proportions.keys(), duplicate_proportions.values())\n",
        "    plt.xlabel('Class')\n",
        "    plt.ylabel('Percentage of Duplicates Removed (%)')\n",
        "    plt.title('Proportion of Duplicated Data per Class')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('duplicate_proportion.png')\n",
        "    plt.show()\n",
        "\n",
        "    return duplicates_found, kept_files, class_counts_before, class_counts_after\n",
        "\n",
        "# Data Loading\n",
        "def load_dataset(image_dir):\n",
        "    images = []\n",
        "    labels = []\n",
        "    class_names = ['bruised', 'cracked', 'rotten', 'spotted', 'unaffected', 'unripe']\n",
        "    for idx, class_name in enumerate(class_names):\n",
        "        class_dir = os.path.join(image_dir, class_name)\n",
        "        if not os.path.exists(class_dir):\n",
        "            print(f\"Directory {class_dir} does not exist\")\n",
        "            continue\n",
        "        for img_file in os.listdir(class_dir):\n",
        "            if img_file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                img_path = os.path.join(class_dir, img_file)\n",
        "                try:\n",
        "                    with Image.open(img_path) as img:\n",
        "                        img.verify()\n",
        "                    images.append(img_path)\n",
        "                    labels.append(idx)\n",
        "                except Exception:\n",
        "                    print(f\"Invalid image: {img_path}\")\n",
        "    return images, labels\n",
        "\n",
        "# Class Balancing with Augmentation\n",
        "def augment_image(image_path, augmentations):\n",
        "    try:\n",
        "        image = cv2.imread(image_path)\n",
        "        if image is None:\n",
        "            raise ValueError(f\"Failed to load image: {image_path}\")\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        augmented = augmentations(image=image)\n",
        "        return augmented['image']\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def balance_classes(images, labels):\n",
        "    if not images or not labels:\n",
        "        print(\"Cannot balance classes: empty dataset\")\n",
        "        return [], []\n",
        "    class_counts = pd.Series(labels).value_counts()\n",
        "    print(\"Class counts before balancing:\")\n",
        "    print(class_counts.rename(index=dict(enumerate(['bruised', 'cracked', 'rotten', 'spotted', 'unaffected', 'unripe']))))\n",
        "\n",
        "    # Plot class distribution before balancing\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    class_counts.plot(kind='bar')\n",
        "    plt.xlabel('Class')\n",
        "    plt.ylabel('Number of Images')\n",
        "    plt.title('Class Distribution Before Balancing')\n",
        "    plt.xticks(ticks=range(len(class_counts)), labels=['bruised', 'cracked', 'rotten', 'spotted', 'unaffected', 'unripe'], rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('class_distribution_before.png')\n",
        "    plt.show()\n",
        "\n",
        "    max_count = class_counts.max()\n",
        "    balanced_images = []\n",
        "    balanced_labels = []\n",
        "    augmentations = A.Compose([\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.Rotate(limit=45, p=0.5),\n",
        "        A.RandomBrightnessContrast(p=0.2),\n",
        "        A.HueSaturationValue(p=0.2),\n",
        "    ])\n",
        "\n",
        "    for cls in range(6):\n",
        "        cls_images = [img for img, lbl in zip(images, labels) if lbl == cls]\n",
        "        num_to_add = max_count - len(cls_images)\n",
        "        if num_to_add > 0:\n",
        "            additional_images = []\n",
        "            for _ in range(num_to_add):\n",
        "                img_path = np.random.choice(cls_images)\n",
        "                aug_img = augment_image(img_path, augmentations)\n",
        "                if aug_img is not None:\n",
        "                    additional_images.append(aug_img)\n",
        "            balanced_images.extend(cls_images)\n",
        "            balanced_images.extend(additional_images[:num_to_add])\n",
        "            balanced_labels.extend([cls] * max_count)\n",
        "        else:\n",
        "            balanced_images.extend(cls_images[:max_count])\n",
        "            balanced_labels.extend([cls] * max_count)\n",
        "\n",
        "    # Plot class distribution after balancing\n",
        "    balanced_counts = pd.Series(balanced_labels).value_counts()\n",
        "    print(\"Class counts after balancing:\")\n",
        "    print(balanced_counts.rename(index=dict(enumerate(['bruised', 'cracked', 'rotten', 'spotted', 'unaffected', 'unripe']))))\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    balanced_counts.plot(kind='bar')\n",
        "    plt.xlabel('Class')\n",
        "    plt.ylabel('Number of Images')\n",
        "    plt.title('Class Distribution After Balancing')\n",
        "    plt.xticks(ticks=range(len(balanced_counts)), labels=['bruised', 'cracked', 'rotten', 'spotted', 'unaffected', 'unripe'], rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('class_distribution_after.png')\n",
        "    plt.show()\n",
        "\n",
        "    return balanced_images, balanced_labels\n",
        "\n",
        "# Display Sample Images\n",
        "def display_sample_images(images, labels, class_names, num_samples=2):\n",
        "    samples_per_class = defaultdict(list)\n",
        "    for img_path, label in zip(images, labels):\n",
        "        if len(samples_per_class[label]) < num_samples:\n",
        "            samples_per_class[label].append(img_path)\n",
        "        if all(len(samples_per_class[l]) >= num_samples for l in range(len(class_names))):\n",
        "            break\n",
        "\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    for idx, (label, img_paths) in enumerate(samples_per_class.items()):\n",
        "        for i, img_path in enumerate(img_paths):\n",
        "            try:\n",
        "                img = Image.open(img_path).convert('RGB')\n",
        "                plt.subplot(len(class_names), num_samples, idx * num_samples + i + 1)\n",
        "                plt.imshow(img)\n",
        "                plt.title(f\"{class_names[label]}\")\n",
        "                plt.axis('off')\n",
        "            except Exception:\n",
        "                plt.subplot(len(class_names), num_samples, idx * num_samples + i + 1)\n",
        "                plt.text(0.5, 0.5, 'Invalid Image', ha='center', va='center')\n",
        "                plt.title(f\"{class_names[label]}\")\n",
        "                plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('sample_images.png')\n",
        "    plt.show()\n",
        "    print(\"Sample images saved to sample_images.png\")\n",
        "\n",
        "# Custom Dataset\n",
        "class PlumDataset(Dataset):\n",
        "    def __init__(self, images, labels, transform=None):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        try:\n",
        "            if isinstance(self.images[idx], str):\n",
        "                img = Image.open(self.images[idx]).convert('RGB')\n",
        "            else:\n",
        "                img = Image.fromarray(self.images[idx])\n",
        "            label = self.labels[idx]\n",
        "            if self.transform:\n",
        "                img = self.transform(img)\n",
        "            return img, label\n",
        "        except Exception:\n",
        "            return None\n",
        "\n",
        "# Modified VGG Model\n",
        "class ModifiedVGG(nn.Module):\n",
        "    def __init__(self, num_classes=6):\n",
        "        super(ModifiedVGG, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512 * 7 * 7, 4096),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# Training Function\n",
        "def train_model(model, train_loader, val_loader, test_loader, criterion, optimizer, swa_model, swa_scheduler, epochs, device, model_path='best_model.pth'):\n",
        "    best_val_loss = float('inf')\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "        for images, labels in train_loader:\n",
        "            if images is None or labels is None:\n",
        "                continue\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            if torch.isnan(loss):\n",
        "                continue\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            train_total += labels.size(0)\n",
        "            train_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        train_loss = train_loss / train_total if train_total > 0 else float('inf')\n",
        "        train_acc = 100 * train_correct / train_total if train_total > 0 else 0\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                if images is None or labels is None:\n",
        "                    continue\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                if torch.isnan(loss):\n",
        "                    continue\n",
        "                val_loss += loss.item() * images.size(0)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_loss = val_loss / val_total if val_total > 0 else float('inf')\n",
        "        val_acc = 100 * val_correct / val_total if val_total > 0 else 0\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | \"\n",
        "              f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "        if val_loss < best_val_loss and val_total > 0:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'val_loss': val_loss,\n",
        "            }, model_path)\n",
        "            print(f\"Saved model checkpoint to {model_path}\")\n",
        "\n",
        "        if epoch >= 10:\n",
        "            swa_model.update_parameters(model)\n",
        "            swa_scheduler.step()\n",
        "\n",
        "    swa_model_path = 'swa_' + model_path\n",
        "    torch.save(swa_model.state_dict(), swa_model_path)\n",
        "    print(f\"Saved SWA model to {swa_model_path}\")\n",
        "\n",
        "    model.eval()\n",
        "    test_loss = 0.0\n",
        "    test_correct = 0\n",
        "    test_total = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            if images is None or labels is None:\n",
        "                continue\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            if torch.isnan(loss):\n",
        "                continue\n",
        "            test_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            test_total += labels.size(0)\n",
        "            test_correct += (predicted == labels).sum().item()\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    test_loss = test_loss / test_total if test_total > 0 else float('inf')\n",
        "    test_acc = 100 * test_correct / test_total if test_total > 0 else 0\n",
        "    print(f\"Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.2f}%\")\n",
        "\n",
        "    if all_labels and all_preds:\n",
        "        cm = confusion_matrix(all_labels, all_preds)\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                    xticklabels=['bruised', 'cracked', 'rotten', 'spotted', 'unaffected', 'unripe'],\n",
        "                    yticklabels=['bruised', 'cracked', 'rotten', 'spotted', 'unaffected', 'unripe'])\n",
        "        plt.xlabel('Predicted')\n",
        "        plt.ylabel('Actual')\n",
        "        plt.title('Confusion Matrix')\n",
        "        plt.savefig('confusion_matrix.png')\n",
        "        plt.show()\n",
        "        print(\"Confusion matrix saved to confusion_matrix.png\")\n",
        "\n",
        "        report = classification_report(all_labels, all_preds,\n",
        "                                     target_names=['bruised', 'cracked', 'rotten', 'spotted', 'unaffected', 'unripe'])\n",
        "        print(\"Classification Report:\\n\" + report)\n",
        "\n",
        "# Main Execution\n",
        "def main():\n",
        "    dataset_dir = kagglehub.dataset_download(\"arnaudfadja/african-plums-quality-and-defect-assessment-data\")\n",
        "    print(f\"Dataset downloaded to: {dataset_dir}\")\n",
        "    print(f\"Contents of dataset_dir: {os.listdir(dataset_dir)}\")\n",
        "\n",
        "    image_dir = os.path.join(dataset_dir, 'african_plums_dataset', 'african_plums')\n",
        "    writable_dir = '/african_plums_cleaned'\n",
        "    os.makedirs(os.path.dirname(writable_dir), exist_ok=True)\n",
        "    if os.path.exists(writable_dir):\n",
        "        shutil.rmtree(writable_dir)\n",
        "    shutil.copytree(image_dir, writable_dir)\n",
        "    print(f\"Copied dataset to: {writable_dir}\")\n",
        "\n",
        "    duplicates_removed, kept_files, class_counts_before, class_counts_after = remove_duplicate_images(writable_dir)\n",
        "    print(f\"Images after duplicate removal: {len(kept_files)}\")\n",
        "\n",
        "    images, labels = load_dataset(writable_dir)\n",
        "    if not images:\n",
        "        print(\"No images found in the dataset. Check directory structure and file extensions.\")\n",
        "        return\n",
        "    print(f\"Initial Dataset: {len(images)} images\")\n",
        "\n",
        "    # Display sample images\n",
        "    class_names = ['bruised', 'cracked', 'rotten', 'spotted', 'unaffected', 'unripe']\n",
        "    display_sample_images(images, labels, class_names)\n",
        "\n",
        "    balanced_images, balanced_labels = balance_classes(images, labels)\n",
        "    if not balanced_images:\n",
        "        print(\"Failed to balance classes. Exiting.\")\n",
        "        return\n",
        "    print(f\"Balanced Dataset: {len(balanced_images)} images\")\n",
        "\n",
        "    train_images, test_images, train_labels, test_labels = train_test_split(\n",
        "        balanced_images, balanced_labels, test_size=0.2, stratify=balanced_labels, random_state=42\n",
        "    )\n",
        "    train_images, val_images, train_labels, val_labels = train_test_split(\n",
        "        train_images, train_labels, test_size=0.125, stratify=train_labels, random_state=42\n",
        "    )\n",
        "    print(f\"Train Set: {len(train_images)} images\")\n",
        "    print(f\"Validation Set: {len(val_images)} images\")\n",
        "    print(f\"Test Set: {len(test_images)} images\")\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((100, 100)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "\n",
        "    train_dataset = PlumDataset(train_images, train_labels, transform=transform)\n",
        "    val_dataset = PlumDataset(val_images, val_labels, transform=transform)\n",
        "    test_dataset = PlumDataset(test_images, test_labels, transform=transform)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = ModifiedVGG(num_classes=6).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=1e-4)\n",
        "    swa_model = AveragedModel(model)\n",
        "    swa_scheduler = SWALR(optimizer, swa_lr=0.0005)\n",
        "\n",
        "    train_model(model, train_loader, val_loader, test_loader, criterion, optimizer, swa_model, swa_scheduler, epochs=100, device=device)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGduCftrtqF2",
        "outputId": "e60ca284-c769-42cf-e409-726f41c05c51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset downloaded to: /kaggle/input/african-plums-quality-and-defect-assessment-data\n",
            "Contents of dataset_dir: ['african_plums_dataset']\n",
            "Copied dataset to: /african_plums_cleaned\n",
            "Total duplicates removed: 1513\n",
            "Images retained: 2994\n",
            "Sample Cleaned Data (images per class):\n",
            "bruised: 118 images\n",
            "cracked: 105 images\n",
            "rotten: 402 images\n",
            "spotted: 611 images\n",
            "unaffected: 1225 images\n",
            "unripe: 533 images\n",
            "Images after duplicate removal: 2994\n",
            "Initial Dataset: 2994 images\n",
            "Sample images saved to sample_images.png\n",
            "Class counts before balancing:\n",
            "unaffected    1225\n",
            "spotted        611\n",
            "unripe         533\n",
            "rotten         402\n",
            "bruised        118\n",
            "cracked        105\n",
            "Name: count, dtype: int64\n",
            "Class counts after balancing:\n",
            "bruised       1225\n",
            "cracked       1225\n",
            "rotten        1225\n",
            "spotted       1225\n",
            "unaffected    1225\n",
            "unripe        1225\n",
            "Name: count, dtype: int64\n",
            "Balanced Dataset: 7350 images\n",
            "Train Set: 5145 images\n",
            "Validation Set: 735 images\n",
            "Test Set: 1470 images\n",
            "Epoch 1/100 | Train Loss: 1.7695 | Train Acc: 21.26% | Val Loss: 1.7001 | Val Acc: 36.05%\n",
            "Saved model checkpoint to best_model.pth\n",
            "Epoch 2/100 | Train Loss: 1.6659 | Train Acc: 32.79% | Val Loss: 1.5770 | Val Acc: 39.73%\n",
            "Saved model checkpoint to best_model.pth\n",
            "Epoch 3/100 | Train Loss: 1.5713 | Train Acc: 37.12% | Val Loss: 1.4914 | Val Acc: 40.82%\n",
            "Saved model checkpoint to best_model.pth\n",
            "Epoch 4/100 | Train Loss: 1.4778 | Train Acc: 42.37% | Val Loss: 1.4125 | Val Acc: 44.49%\n",
            "Saved model checkpoint to best_model.pth\n",
            "Epoch 5/100 | Train Loss: 1.4055 | Train Acc: 46.53% | Val Loss: 1.3473 | Val Acc: 47.21%\n",
            "Saved model checkpoint to best_model.pth\n",
            "Epoch 6/100 | Train Loss: 1.3318 | Train Acc: 49.52% | Val Loss: 1.3062 | Val Acc: 49.52%\n",
            "Saved model checkpoint to best_model.pth\n",
            "Epoch 7/100 | Train Loss: 1.2712 | Train Acc: 52.58% | Val Loss: 1.2450 | Val Acc: 50.20%\n",
            "Saved model checkpoint to best_model.pth\n",
            "Epoch 8/100 | Train Loss: 1.2120 | Train Acc: 53.90% | Val Loss: 1.2193 | Val Acc: 50.75%\n",
            "Saved model checkpoint to best_model.pth\n",
            "Epoch 9/100 | Train Loss: 1.1553 | Train Acc: 56.38% | Val Loss: 1.1703 | Val Acc: 52.93%\n",
            "Saved model checkpoint to best_model.pth\n",
            "Epoch 10/100 | Train Loss: 1.1014 | Train Acc: 58.56% | Val Loss: 1.1472 | Val Acc: 55.10%\n",
            "Saved model checkpoint to best_model.pth\n",
            "Epoch 11/100 | Train Loss: 1.0495 | Train Acc: 61.89% | Val Loss: 1.1015 | Val Acc: 56.46%\n",
            "Saved model checkpoint to best_model.pth\n",
            "Epoch 12/100 | Train Loss: 1.0030 | Train Acc: 63.13% | Val Loss: 1.0721 | Val Acc: 57.14%\n",
            "Saved model checkpoint to best_model.pth\n",
            "Epoch 13/100 | Train Loss: 0.9523 | Train Acc: 65.52% | Val Loss: 1.0573 | Val Acc: 59.32%\n",
            "Saved model checkpoint to best_model.pth\n",
            "Epoch 14/100 | Train Loss: 0.8979 | Train Acc: 67.27% | Val Loss: 1.0314 | Val Acc: 58.64%\n",
            "Saved model checkpoint to best_model.pth\n",
            "Epoch 15/100 | Train Loss: 0.8477 | Train Acc: 69.41% | Val Loss: 0.9970 | Val Acc: 61.09%\n",
            "Saved model checkpoint to best_model.pth\n",
            "Epoch 16/100 | Train Loss: 0.8036 | Train Acc: 71.02% | Val Loss: 0.9761 | Val Acc: 63.81%\n",
            "Saved model checkpoint to best_model.pth\n",
            "Epoch 17/100 | Train Loss: 0.7663 | Train Acc: 73.10% | Val Loss: 0.9364 | Val Acc: 65.31%\n",
            "Saved model checkpoint to best_model.pth\n",
            "Epoch 18/100 | Train Loss: 0.7203 | Train Acc: 74.75% | Val Loss: 0.9207 | Val Acc: 65.71%\n",
            "Saved model checkpoint to best_model.pth\n",
            "Epoch 19/100 | Train Loss: 0.6813 | Train Acc: 77.55% | Val Loss: 0.8805 | Val Acc: 66.94%\n",
            "Saved model checkpoint to best_model.pth\n",
            "Epoch 20/100 | Train Loss: 0.6505 | Train Acc: 78.17% | Val Loss: 0.8734 | Val Acc: 68.57%\n",
            "Saved model checkpoint to best_model.pth\n",
            "Epoch 21/100 | Train Loss: 0.6181 | Train Acc: 79.03% | Val Loss: 0.9083 | Val Acc: 66.94%\n",
            "Epoch 22/100 | Train Loss: 0.5716 | Train Acc: 81.44% | Val Loss: 0.9092 | Val Acc: 65.44%\n",
            "Epoch 23/100 | Train Loss: 0.5643 | Train Acc: 81.48% | Val Loss: 0.8396 | Val Acc: 70.48%\n",
            "Saved model checkpoint to best_model.pth\n",
            "Epoch 24/100 | Train Loss: 0.5305 | Train Acc: 82.90% | Val Loss: 0.8419 | Val Acc: 70.61%\n",
            "Epoch 25/100 | Train Loss: 0.5010 | Train Acc: 84.14% | Val Loss: 0.8475 | Val Acc: 70.48%\n",
            "Epoch 26/100 | Train Loss: 0.4774 | Train Acc: 84.96% | Val Loss: 0.8363 | Val Acc: 71.56%\n",
            "Saved model checkpoint to best_model.pth\n",
            "Epoch 27/100 | Train Loss: 0.4355 | Train Acc: 86.98% | Val Loss: 0.8626 | Val Acc: 69.93%\n",
            "Epoch 28/100 | Train Loss: 0.4237 | Train Acc: 87.64% | Val Loss: 0.8207 | Val Acc: 71.29%\n",
            "Saved model checkpoint to best_model.pth\n",
            "Epoch 29/100 | Train Loss: 0.3952 | Train Acc: 88.57% | Val Loss: 0.7953 | Val Acc: 72.24%\n",
            "Saved model checkpoint to best_model.pth\n",
            "Epoch 30/100 | Train Loss: 0.3857 | Train Acc: 87.95% | Val Loss: 0.7978 | Val Acc: 72.65%\n",
            "Epoch 31/100 | Train Loss: 0.3568 | Train Acc: 89.60% | Val Loss: 0.8064 | Val Acc: 72.24%\n",
            "Epoch 32/100 | Train Loss: 0.3410 | Train Acc: 90.32% | Val Loss: 0.7631 | Val Acc: 72.79%\n",
            "Saved model checkpoint to best_model.pth\n",
            "Epoch 33/100 | Train Loss: 0.3092 | Train Acc: 91.88% | Val Loss: 0.7819 | Val Acc: 73.74%\n",
            "Epoch 34/100 | Train Loss: 0.2966 | Train Acc: 91.88% | Val Loss: 0.7996 | Val Acc: 73.88%\n",
            "Epoch 35/100 | Train Loss: 0.2830 | Train Acc: 92.15% | Val Loss: 0.7799 | Val Acc: 73.74%\n",
            "Epoch 36/100 | Train Loss: 0.2607 | Train Acc: 93.06% | Val Loss: 0.8025 | Val Acc: 72.79%\n",
            "Epoch 37/100 | Train Loss: 0.2493 | Train Acc: 93.47% | Val Loss: 0.7796 | Val Acc: 74.56%\n",
            "Epoch 38/100 | Train Loss: 0.2232 | Train Acc: 94.34% | Val Loss: 0.8025 | Val Acc: 74.01%\n",
            "Epoch 39/100 | Train Loss: 0.2167 | Train Acc: 94.77% | Val Loss: 0.7595 | Val Acc: 75.92%\n",
            "Saved model checkpoint to best_model.pth\n",
            "Epoch 40/100 | Train Loss: 0.1977 | Train Acc: 95.22% | Val Loss: 0.7810 | Val Acc: 75.10%\n",
            "Epoch 41/100 | Train Loss: 0.1883 | Train Acc: 95.45% | Val Loss: 0.8538 | Val Acc: 72.93%\n",
            "Epoch 42/100 | Train Loss: 0.1760 | Train Acc: 95.86% | Val Loss: 0.8508 | Val Acc: 72.93%\n",
            "Epoch 43/100 | Train Loss: 0.1653 | Train Acc: 96.25% | Val Loss: 0.7739 | Val Acc: 75.10%\n",
            "Epoch 44/100 | Train Loss: 0.1585 | Train Acc: 96.29% | Val Loss: 0.8289 | Val Acc: 75.10%\n",
            "Epoch 45/100 | Train Loss: 0.1346 | Train Acc: 97.55% | Val Loss: 0.8579 | Val Acc: 73.61%\n",
            "Epoch 46/100 | Train Loss: 0.1270 | Train Acc: 97.28% | Val Loss: 0.8908 | Val Acc: 73.20%\n",
            "Epoch 47/100 | Train Loss: 0.1229 | Train Acc: 97.18% | Val Loss: 0.8271 | Val Acc: 75.24%\n",
            "Epoch 48/100 | Train Loss: 0.1112 | Train Acc: 97.80% | Val Loss: 0.8755 | Val Acc: 72.11%\n",
            "Epoch 49/100 | Train Loss: 0.1087 | Train Acc: 97.65% | Val Loss: 0.8303 | Val Acc: 75.65%\n",
            "Epoch 50/100 | Train Loss: 0.1036 | Train Acc: 97.82% | Val Loss: 1.0055 | Val Acc: 70.48%\n",
            "Epoch 51/100 | Train Loss: 0.0986 | Train Acc: 98.04% | Val Loss: 0.8765 | Val Acc: 74.56%\n",
            "Epoch 52/100 | Train Loss: 0.0876 | Train Acc: 98.50% | Val Loss: 0.9411 | Val Acc: 73.47%\n",
            "Epoch 53/100 | Train Loss: 0.0888 | Train Acc: 98.23% | Val Loss: 0.9154 | Val Acc: 74.29%\n",
            "Epoch 54/100 | Train Loss: 0.0744 | Train Acc: 98.74% | Val Loss: 0.8710 | Val Acc: 76.19%\n",
            "Epoch 55/100 | Train Loss: 0.0728 | Train Acc: 98.64% | Val Loss: 0.9074 | Val Acc: 75.24%\n",
            "Epoch 56/100 | Train Loss: 0.0655 | Train Acc: 98.79% | Val Loss: 1.0437 | Val Acc: 72.11%\n",
            "Epoch 57/100 | Train Loss: 0.0594 | Train Acc: 98.97% | Val Loss: 0.9217 | Val Acc: 74.97%\n",
            "Epoch 58/100 | Train Loss: 0.0637 | Train Acc: 98.72% | Val Loss: 0.9040 | Val Acc: 74.97%\n",
            "Epoch 59/100 | Train Loss: 0.0577 | Train Acc: 98.74% | Val Loss: 0.9399 | Val Acc: 74.97%\n",
            "Epoch 60/100 | Train Loss: 0.0565 | Train Acc: 98.70% | Val Loss: 1.0626 | Val Acc: 72.65%\n",
            "Epoch 61/100 | Train Loss: 0.0485 | Train Acc: 99.09% | Val Loss: 1.1020 | Val Acc: 73.20%\n",
            "Epoch 62/100 | Train Loss: 0.0475 | Train Acc: 99.26% | Val Loss: 0.9638 | Val Acc: 75.37%\n",
            "Epoch 63/100 | Train Loss: 0.0463 | Train Acc: 99.16% | Val Loss: 0.9769 | Val Acc: 75.51%\n",
            "Epoch 64/100 | Train Loss: 0.0424 | Train Acc: 99.26% | Val Loss: 0.9436 | Val Acc: 75.78%\n",
            "Epoch 65/100 | Train Loss: 0.0401 | Train Acc: 99.22% | Val Loss: 1.0404 | Val Acc: 73.88%\n",
            "Epoch 66/100 | Train Loss: 0.0355 | Train Acc: 99.34% | Val Loss: 0.9882 | Val Acc: 74.69%\n",
            "Epoch 67/100 | Train Loss: 0.0399 | Train Acc: 99.13% | Val Loss: 0.9807 | Val Acc: 75.92%\n",
            "Epoch 68/100 | Train Loss: 0.0367 | Train Acc: 99.14% | Val Loss: 1.0858 | Val Acc: 75.24%\n",
            "Epoch 69/100 | Train Loss: 0.0326 | Train Acc: 99.26% | Val Loss: 0.9786 | Val Acc: 77.69%\n",
            "Epoch 70/100 | Train Loss: 0.0314 | Train Acc: 99.32% | Val Loss: 0.9869 | Val Acc: 74.69%\n",
            "Epoch 71/100 | Train Loss: 0.0304 | Train Acc: 99.40% | Val Loss: 0.9868 | Val Acc: 76.19%\n",
            "Epoch 72/100 | Train Loss: 0.0272 | Train Acc: 99.36% | Val Loss: 1.0426 | Val Acc: 75.51%\n",
            "Epoch 73/100 | Train Loss: 0.0282 | Train Acc: 99.40% | Val Loss: 1.0417 | Val Acc: 77.01%\n",
            "Epoch 74/100 | Train Loss: 0.0251 | Train Acc: 99.53% | Val Loss: 1.3214 | Val Acc: 72.24%\n",
            "Epoch 75/100 | Train Loss: 0.0261 | Train Acc: 99.44% | Val Loss: 1.1744 | Val Acc: 75.24%\n",
            "Epoch 76/100 | Train Loss: 0.0234 | Train Acc: 99.53% | Val Loss: 1.0664 | Val Acc: 77.55%\n",
            "Epoch 77/100 | Train Loss: 0.0174 | Train Acc: 99.73% | Val Loss: 1.1498 | Val Acc: 75.51%\n",
            "Epoch 78/100 | Train Loss: 0.0275 | Train Acc: 99.24% | Val Loss: 1.1013 | Val Acc: 75.51%\n",
            "Epoch 79/100 | Train Loss: 0.0216 | Train Acc: 99.40% | Val Loss: 1.1128 | Val Acc: 76.05%\n",
            "Epoch 80/100 | Train Loss: 0.0188 | Train Acc: 99.48% | Val Loss: 1.0801 | Val Acc: 75.51%\n",
            "Epoch 81/100 | Train Loss: 0.0149 | Train Acc: 99.75% | Val Loss: 1.2227 | Val Acc: 74.29%\n",
            "Epoch 82/100 | Train Loss: 0.0210 | Train Acc: 99.46% | Val Loss: 1.2936 | Val Acc: 74.29%\n",
            "Epoch 83/100 | Train Loss: 0.0138 | Train Acc: 99.69% | Val Loss: 1.2353 | Val Acc: 74.83%\n",
            "Epoch 84/100 | Train Loss: 0.0180 | Train Acc: 99.59% | Val Loss: 1.1614 | Val Acc: 76.05%\n",
            "Epoch 85/100 | Train Loss: 0.0139 | Train Acc: 99.61% | Val Loss: 1.2439 | Val Acc: 74.69%\n",
            "Epoch 86/100 | Train Loss: 0.0157 | Train Acc: 99.67% | Val Loss: 1.2733 | Val Acc: 75.10%\n",
            "Epoch 87/100 | Train Loss: 0.0127 | Train Acc: 99.75% | Val Loss: 1.2714 | Val Acc: 75.65%\n",
            "Epoch 88/100 | Train Loss: 0.0158 | Train Acc: 99.63% | Val Loss: 1.1529 | Val Acc: 76.60%\n",
            "Epoch 89/100 | Train Loss: 0.0105 | Train Acc: 99.77% | Val Loss: 1.2411 | Val Acc: 75.37%\n",
            "Epoch 90/100 | Train Loss: 0.0112 | Train Acc: 99.69% | Val Loss: 1.1561 | Val Acc: 76.73%\n",
            "Epoch 91/100 | Train Loss: 0.0112 | Train Acc: 99.75% | Val Loss: 1.2695 | Val Acc: 75.51%\n",
            "Epoch 92/100 | Train Loss: 0.0143 | Train Acc: 99.73% | Val Loss: 1.2899 | Val Acc: 77.14%\n",
            "Epoch 93/100 | Train Loss: 0.0094 | Train Acc: 99.83% | Val Loss: 1.2696 | Val Acc: 76.60%\n",
            "Epoch 94/100 | Train Loss: 0.0129 | Train Acc: 99.67% | Val Loss: 1.2947 | Val Acc: 74.29%\n",
            "Epoch 95/100 | Train Loss: 0.0121 | Train Acc: 99.69% | Val Loss: 1.3068 | Val Acc: 75.24%\n",
            "Epoch 96/100 | Train Loss: 0.0073 | Train Acc: 99.88% | Val Loss: 1.2905 | Val Acc: 76.05%\n",
            "Epoch 97/100 | Train Loss: 0.0102 | Train Acc: 99.77% | Val Loss: 1.2843 | Val Acc: 76.73%\n",
            "Epoch 98/100 | Train Loss: 0.0067 | Train Acc: 99.86% | Val Loss: 1.2703 | Val Acc: 77.96%\n",
            "Epoch 99/100 | Train Loss: 0.0058 | Train Acc: 99.90% | Val Loss: 1.3827 | Val Acc: 76.19%\n",
            "Epoch 100/100 | Train Loss: 0.0132 | Train Acc: 99.65% | Val Loss: 1.2730 | Val Acc: 76.60%\n",
            "Saved SWA model to swa_best_model.pth\n",
            "Test Loss: 1.3502 | Test Acc: 75.03%\n",
            "Confusion matrix saved to confusion_matrix.png\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     bruised       0.88      0.71      0.79       245\n",
            "     cracked       0.85      0.88      0.87       245\n",
            "      rotten       0.86      0.64      0.74       245\n",
            "     spotted       0.53      0.67      0.59       245\n",
            "  unaffected       0.75      0.84      0.79       245\n",
            "      unripe       0.73      0.76      0.74       245\n",
            "\n",
            "    accuracy                           0.75      1470\n",
            "   macro avg       0.77      0.75      0.75      1470\n",
            "weighted avg       0.77      0.75      0.75      1470\n",
            "\n"
          ]
        }
      ]
    }
  ]
}
